[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CalCOFI.io Docs",
    "section": "",
    "text": "1 Process\n\n\n\n\n\n\n\nFigure 1.1: CalCOFI data workflow.\n\n\n\nThe original raw data, most often in tabular format [e.g., comma-separated value (*.csv)], gets ingested into the database by R scripts that use functions and lookup data tables in the R package calcofi4r where functions are organized into Read, Analyze and Visualize concepts. The application programming interface (API) provides a program-language-agnostic public interface for rendering subsets of data and custom visualizations given a set of documented input parameters for feeding interactive applications (Apps) using Shiny (or any other web application framework) and reports using Rmarkdown (or any other report templating framework). Finally, R scripts will publish metadata (as Ecological Metadata Language) and data packages (e.g., in Darwin format) for discovery on a variety of data portals oriented around slicing the tabular or gridded data (ERDDAP), biogeographic analysis (OBIS), long-term archive (DataOne, NCEI) or metadata discovery (InPort). The database will be spatially enabled by PostGIS for summarizing any and all data by Areas of Interest (AoIs), whether pre-defined (e.g., sanctuaries, MPAs, counties, etc.) or arbitrary new areas. (Figure 1.1)\n\nERDDAP: great for gridded or tabular data, but does not aggregate on the server or clip to a specific area of interest",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process</span>"
    ]
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "2  Reports",
    "section": "",
    "text": "2.1 Sanctuaries",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "reports.html#sanctuaries",
    "href": "reports.html#sanctuaries",
    "title": "2  Reports",
    "section": "",
    "text": "Channel Islands WebCR\nweb-enabled Condition Report\n\nForage Fish\nexample of using calcofi4r functions that pull from the API\n\nUCSB Student Capstone",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "3  Applications",
    "section": "",
    "text": "CalCOFI Oceanography\noceanographic summarization by arbitrary area of interest and sampling period\nUCSB Student Capstone",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "4  API",
    "section": "",
    "text": "4.1 /variables: get list of variables for timeseries\nThe raw interface to the Application Programming Interface (API) is available at:\nHere we will provide more guidance on how to use the API functions with documented input arguments, output results and examples of use.\nGet list of variables for use in /timeseries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#species_groups-get-species-groups-for-larvae",
    "href": "api.html#species_groups-get-species-groups-for-larvae",
    "title": "4  API",
    "section": "4.2 /species_groups: get species groups for larvae",
    "text": "4.2 /species_groups: get species groups for larvae\nNot yet working. Get list of species groups for use with variables larvae_counts.count in /timeseries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#timeseries-get-time-series-data",
    "href": "api.html#timeseries-get-time-series-data",
    "title": "4  API",
    "section": "4.3 /timeseries: get time series data",
    "text": "4.3 /timeseries: get time series data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruises-get-list-of-cruises",
    "href": "api.html#cruises-get-list-of-cruises",
    "title": "4  API",
    "section": "4.4 /cruises: get list of cruises",
    "text": "4.4 /cruises: get list of cruises\nGet list of cruises with summary stats as CSV table for time (date_beg)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#raster-get-raster-map-of-variable",
    "href": "api.html#raster-get-raster-map-of-variable",
    "title": "4  API",
    "section": "4.5 /raster: get raster map of variable",
    "text": "4.5 /raster: get raster map of variable\nGet raster of variable",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruise_lines-get-station-lines-from-cruises",
    "href": "api.html#cruise_lines-get-station-lines-from-cruises",
    "title": "4  API",
    "section": "4.6 /cruise_lines: get station lines from cruises",
    "text": "4.6 /cruise_lines: get station lines from cruises\nGet station lines from cruises (with more than one cast)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruise_line_profile",
    "href": "api.html#cruise_line_profile",
    "title": "4  API",
    "section": "4.7 /cruise_line_profile",
    "text": "4.7 /cruise_line_profile\nGet profile at depths for given variable of casts along line of stations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "db.html",
    "href": "db.html",
    "title": "5  Database",
    "section": "",
    "text": "5.1 Database naming conventions\nWe’re circling the wagons to come up with the best conventions for naming. Here are some ideas:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#database-naming-conventions",
    "href": "db.html#database-naming-conventions",
    "title": "5  Database",
    "section": "",
    "text": "There are only two hard things in Computer Science: cache invalidation and naming things. – Phil Karlton (Netscape architect)\n\n\n\nLearn SQL: Naming Conventions\nBest Practices for Database Naming Conventions - Drygast.NET\n\n\n5.1.1 Name tables\n\nTable names are singular and use all lower case.\n\n\n\n5.1.2 Name columns\n\nTo name columns, use snake-case (i.e., lower-case with underscores) so as to prevent the need to quote SQL statements. (TIP: Use janitor::clean_names() to convert a table.)\nUnique identifiers are suffixed with:\n\n*_id for unique integer keys;\n*_uuid for universally unique identifiers as defined by RFC 4122 and stored in Postgres as UUID Type.\n*_key for unique string keys;\n*_seq for auto-incrementing sequence integer keys.\n\nSuffix with units where applicable (e.g., *_m for meters, *_km for kilometers, degc for degrees Celsius). See units vignette.\nSet geometry column to geom (used by PostGIS spatial extension). If the table has multiple geometry columns, use geom for the default geometry column and geom_{type} for additional geometry columns (e.g., geom_point, geom_line, geom_polygon).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#use-unicode-for-text",
    "href": "db.html#use-unicode-for-text",
    "title": "5  Database",
    "section": "5.2 Use Unicode for text",
    "text": "5.2 Use Unicode for text\nThe default character encoding for Postgresql is unicode (UTF8), which allows for international characters, accents and special characters. Improper encoding can royally mess up basic text.\nLogging into the server, we can see this with the following command:\ndocker exec -it postgis psql -l\n                                  List of databases\n        Name        | Owner | Encoding |  Collate   |   Ctype    | Access privileges \n--------------------+-------+----------+------------+------------+-------------------\n gis                | admin | UTF8     | en_US.utf8 | en_US.utf8 | =Tc/admin        +\n                    |       |          |            |            | admin=CTc/admin  +\n                    |       |          |            |            | ro_user=c/admin\n lter_core_metabase | admin | UTF8     | en_US.utf8 | en_US.utf8 | =Tc/admin        +\n                    |       |          |            |            | admin=CTc/admin  +\n                    |       |          |            |            | rw_user=c/admin\n postgres           | admin | UTF8     | en_US.utf8 | en_US.utf8 | \n template0          | admin | UTF8     | en_US.utf8 | en_US.utf8 | =c/admin         +\n                    |       |          |            |            | admin=CTc/admin\n template1          | admin | UTF8     | en_US.utf8 | en_US.utf8 | =c/admin         +\n                    |       |          |            |            | admin=CTc/admin\n template_postgis   | admin | UTF8     | en_US.utf8 | en_US.utf8 | \n(6 rows)\nUse Unicode (utf-8 in Python or UTF8 in Postgresql) encoding for all database text values to support international characters and documentation (i.e., tabs, etc for markdown conversion).\n\nIn Python, use pandas to read (read_csv()) and write (to_csv()) with UTF-8 encoding (i.e., encoding='utf-8').:\nimport pandas as pd\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://user:password@localhost:5432/dbname')\n\n# read from a csv file\ndf = pd.read_csv('file.csv', encoding='utf-8')\n\n# write to PostgreSQL\ndf.to_sql('table_name', engine, if_exists='replace', index=False, method='multi', chunksize=1000, encoding='utf-8')\n\n# read from PostgreSQL\ndf = pd.read_sql('SELECT * FROM table_name', engine, encoding='utf-8')\n\n# write to a csv file with UTF-8 encoding\ndf.to_csv('file.csv', index=False, encoding='utf-8')\nIn R, use readr to read (read_csv()) and write (write_excel_csv()) to force UTF-8 encoding.\nlibrary(readr)\nlibrary(DBI)\nlibrary(RPostgres)\n\n# connect to PostgreSQL\ncon &lt;- dbConnect(RPostgres::Postgres(), dbname = \"dbname\", host = \"localhost\", port = 5432, user = \"user\", password = \"password\")\n\n# read from a csv file\ndf &lt;- read_csv('file.csv', locale = locale(encoding = 'UTF-8'))  # explicit\ndf &lt;- read_csv('file.csv')                                       # implicit\n\n# write to PostgreSQL\ndbWriteTable(con, 'table_name', df, overwrite = TRUE)\n\n# read from PostgreSQL\ndf &lt;- dbReadTable(con, 'table_name')\n\n# write to a csv file with UTF-8 encoding\nwrite_excel_csv(df, 'file.csv', locale = locale(encoding = 'UTF-8'))  # explicit\nwrite_excel_csv(df, 'file.csv')                                       # implicit",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#ingest-datasets-with-documentation",
    "href": "db.html#ingest-datasets-with-documentation",
    "title": "5  Database",
    "section": "5.3 Ingest datasets with documentation",
    "text": "5.3 Ingest datasets with documentation\nUse Quarto documents with chunks of R code in the workflows Github repository to ingest datasets into the database. For example, see the ingest_noaa-calcofi-db workflow.\n\n\n\n\n\n\nflowchart TB\n    %% Node definitions\n    gd[(\"`&lt;b&gt;Source Data&lt;/b&gt;\n          Google Drive:\n          calcofi/data/{dataset}/*.csv`\")]\n    iw[\"&lt;b&gt;Ingest Workflow&lt;/b&gt;\n        workflows: ingest_{dataset}.qmd\"]\n    dd[\"&lt;b&gt;Data Definitions&lt;/b&gt;\n        workflows: /ingest/{dataset}/:\n        &lt;ul&gt;\n          &lt;li&gt;tbls_redefine.csv&lt;/li&gt;\n          &lt;li&gt;flds_redefine.csv&lt;/li&gt;\n        &lt;/ul&gt;\"]\n    db[(\"&lt;b&gt;Database&lt;/b&gt;\")]\n    api[\"&lt;b&gt;API Endpoint&lt;/b&gt;\\n/db_tables\"]\n    catalog[\"&lt;b&gt;R Function&lt;/b&gt;\\ncalcofi4r::cc_db_catalog()\"]\n    eml[\"&lt;b&gt;Publish Workflow&lt;/b&gt;\n      workflows: publish_{dataset}_{portal}.qmd\n      with {portal}s:\n      &lt;ul&gt;\n        &lt;li&gt;erddap&lt;/li&gt;\n        &lt;li&gt;edi&lt;/li&gt;\n        &lt;li&gt;obis&lt;/li&gt;\n        &lt;li&gt;ncei&lt;/li&gt;\n      &lt;/ul&gt;\"]\n\n    %% Edge definitions\n    gd --&gt; iw\n    iw --&gt;|\"1. auto-generated\"| dd\n    dd --&gt;|\"2. manual edit\"| iw\n    iw --&gt;|\"3. data\"| db\n    iw --&gt; comments\n    comments --&gt;|\"4. metadata\"| db\n    db --&gt; api\n    api --&gt; catalog\n    db --&gt; eml\n\n    %% Comments subgraph with internal nodes\n    subgraph comments[\"&lt;b&gt;Database Comments&lt;/b&gt;\n              (stored as text in JSON format to differentiate elements)\"]\n        direction TB\n        h[\"hideme\"]:::hidden\n        h~~~tbl\n        h~~~fld\n        tbl[\"per &lt;em&gt;Table&lt;/em&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;description&lt;/li&gt;\n              &lt;li&gt;source (&lt;em&gt;linked&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;source_created (&lt;em&gt;datetime&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;workflow (&lt;em&gt;linked&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;workflow_ingested (&lt;em&gt;datetime&lt;/em&gt;)&lt;/li&gt;\n            &lt;/ul&gt;\"]\n        fld[\"per &lt;em&gt;Field&lt;/em&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;description&lt;/li&gt;\n              &lt;li&gt;units (SI)`&lt;/li&gt;\n            &lt;/ul&gt;\"]\n    end\n\n    %% Clickable links\n    click gd \"https://drive.google.com/drive/folders/1xxdWa4mWkmfkJUQsHxERTp9eBBXBMbV7\" \"calcofi folder - Google Drive\"\n    click api \"https://api.calcofi.io/db_tables\" \"API endpoint&lt;/b&gt;\"\n    click catalog \"https://calcofi.io/calcofi4r/reference/cc_db_catalog.html\" \"R package function\"\n\n    %% Styling\n    classDef source fill:#f9f9f9,stroke:#000,stroke-width:2px,color:#000\n    classDef process fill:#a3e0f2,stroke:#000,stroke-width:2px,color:#000\n    classDef eml fill:#F0FDF4,stroke:#22C55E,stroke-width:2px,color:#000,text-align:left\n    classDef data fill:#ffbe75,stroke:#000,stroke-width:2px,color:#000\n    classDef api fill:#9ad294,stroke:#000,stroke-width:2px,color:#000\n    classDef meta fill:#c9a6db,stroke:#000,stroke-width:2px,color:#000,text-align:left\n    classDef hidden display: none;\n\n    class gd source\n    class dd,comments,tbl,fld meta\n    class iw process\n    class db data\n    class api,catalog api\n    class tbl,fld li\n    class eml eml\n\n\n\n\nFigure 5.1: Database documentation scheme.\n\n\n\n\n\nGoogle Drive *.csv files get ingested with a workflow per dataset (in Github repository calcofi/workflows as a Quarto document). Data definition CSV files (tbls_redefine.csv , flds_redefine.csv) are auto-generated (if missing) and manually updated to rename and describe tables and fields. After injecting the data for each of the tables, extra metadata is added to the COMMENTs of each table as JSON elements (links in markdown), including at the table level:\n\ndescription: general description describing contents and how each row is unique\nsource: CSV (linked to Google Drive source as markdown)\nsource_created: datetime stamp of when source was created on GoogleDrive\nworkflow: html (rendered Quarto document on Github)\nworkflow_ingested: datetime of ingestion\n\nAnd at the field level:\n\ndescription: general description of the field\nunits: using the International System of Units (SI) as much as possible\n\nThese comments are then exposed by the API db_tables endpoint, which can be consumed and rendered into a tabular searchable catalog with calcofi4r::cc_db_catalog.\nAdditional workflows will publish the data to the various Portals (ERDDAP, EDI, OBIS, NCEI) using ecological metadata language (EML) and the EML R package, pulling directly from the structured metadata in the database (on table and field definitions).\n\n5.3.1 OR Describe tables and columns directly\n\nUse the COMMENT clause to add descriptions to tables and columns, either through the GUI pgadmin.calcofi.io (by right-clicking on the table or column and selecting Properties) or with SQL. For example:\nCOMMENT ON TABLE public.aoi_fed_sanctuaries IS 'areas of interest (`aoi`) polygons for federal **National Marine Sanctuaries**; loaded by _workflow_ [load_sanctuaries](https://calcofi.io/workflows/load_sanctuaries.html)';\nNote the use of markdown for including links and formatting (e.g., bold, code, italics), such that the above SQL will render like so:\n\nareas of interest (aoi) polygons for federal National Marine Sanctuaries; loaded by workflow load_sanctuaries\n\nIt is especially helpful to link to any workflows that are responsible for the ingesting or updating of the input data.\n\n\n\n5.3.2 Display tables and columns with metadata\n\nThese descriptions can be viewed in the CalCOFI API api.calcofi.io as CSV tables (see code in calcofi/api: plumber.R):\n\napi.calcofi.io/db_tables\nfields:\n\n\nschema: (only “public” so far)\ntable_type: “table”, “view”, or “materialized view” (none yet)\ntable: name of table\ntable_description: description of table (possibly in markdown)\n\napi.calcofi.io/db_columns\nfields:\n\n\nschema: (only “public” so far)\ntable_type: “table”, “view”, or “materialized view” (none yet)\ntable: name of table\ncolumn: name of column\ncolumn_type: data type of column\ncolumn_description: description of column (possibly in markdown)\n\n\nFetch and display these descriptions into an interactive table with calcofi4r::cc_db_catalog().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#relationships-between-tables",
    "href": "db.html#relationships-between-tables",
    "title": "5  Database",
    "section": "5.4 Relationships between tables",
    "text": "5.4 Relationships between tables\n\nSee calcofi/workflows: clean_db\n\nTODO: add calcofi/apps: db to show latest tables, columns and relationsips",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#spatial-tips",
    "href": "db.html#spatial-tips",
    "title": "5  Database",
    "section": "5.5 Spatial Tips",
    "text": "5.5 Spatial Tips\n\nUse ST_Subdivide() when running spatial joins on large polygons.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "portals.html",
    "href": "portals.html",
    "title": "6  Portals",
    "section": "",
    "text": "6.1 Overview\nCalCOFI data is available through various portals, each serving different purposes and user needs. This document outlines the main access points and their characteristics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#data-flow",
    "href": "portals.html#data-flow",
    "title": "6  Portals",
    "section": "6.2 Data Flow",
    "text": "6.2 Data Flow\nWhile it would be ideal for CalCOFI data to be available through a single portal, each portal has its strengths and limitations. The following diagram illustrates one possible realization of data flow between CalCOFI data and the portals: from raw data to the integrated database to portals and meta-portals.\n\n\n\n\n\n\ngraph TD\n  %% nodes with styles\n  raw([raw data]):::source\n\n  subgraph calcofi[CalCOFI.io]\n    db[(database)]:::database\n    web[CalCOFI.org]:::website\n    api[APIs]:::api\n    lib[library]:::code\n    app[apps]:::code\n    flow[workflows]:::code\n  end\n\n  subgraph portals[Portals]\n    edi[EDI]:::portal\n    obis[OBIS]:::portal\n    erddap[ERDDAP]:::portal\n    ncei[NCEI]:::portal\n  end\n\n  subgraph meta[Meta-Portals]\n    odis[ODIS]:::metaportal\n    datagov[data.gov]:::metaportal\n  end\n\n  %% edges\n  raw --&gt; db\n  db  --&gt; api\n  db  --&gt; web\n  api --&gt; lib\n  api --&gt; app\n  api --&gt; flow\n  flow  --&gt; portals\n  portals --&gt;|sitemap| odis\n  portals --&gt; datagov\n\n  %% Custom styles\n  classDef source     fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n  classDef database   fill:#FEF3C7,stroke:#D97706,stroke-width:2px\n  classDef website    fill:#F3E8FF,stroke:#9333EA,stroke-width:2px\n  classDef api        fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n  classDef code       fill:#DBEAFE,stroke:#3B82F6,stroke-width:2px\n  classDef portal     fill:#F0FDF4,stroke:#22C55E,stroke-width:2px\n  classDef metaportal fill:#FEF2F2,stroke:#DC2626,stroke-width:2px\n\n  %% Style subgraphs\n  style calcofi fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n  style portals fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n  style meta fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n\n\n\n\nFigure 6.1: Flow of data from raw to database to portals and meta-portals.\n\n\n\n\n\nIn practice, CalCOFI is a partnership with various contributing members, so the authoritative dataset might flow differently, such as from EDI to the database to the other portals. The other portals, such as OBIS or ERDDAP, serve different audiences or purposes. The meta-portals like ODIS and Data.gov then index these portals to provide broader discovery of CalCOFI datasets.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#portals",
    "href": "portals.html#portals",
    "title": "6  Portals",
    "section": "6.3 Portals",
    "text": "6.3 Portals\nWhile some portals serve as data repositories, others provide advanced data access and visualization tools. The following sections describe the main portals where CalCOFI data is available and their key features.\n\n\n\n\nTable 6.1: Portal Capabilities.\n\n\n\n\n\n\n\n\n\n\nFull Archive\nVersioning\nDOI Issued\nQuery by xyt\nQuery by taxa\nMultiple formats\nAPI Access\n\n\n\n\nEDI\n✔\n✔\n✔\n▲\n▲\n✖\n▲\n\n\nNCEI\n✔\n✔\n✔\n✖\n✖\n✖\n▲\n\n\nOBIS\n▲\n▲\n▲\n✔\n✔\n▲\n✔\n\n\nERDDAP\n▲\n✖\n✖\n✔\n▲\n✔\n✔\n\n\n\nCapability Legend: ✔ = full, ▲ = partial, ✖ = none\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.1 EDI\nEnvironmental Data Initiative\n\nComplete dataset archives using DataOne software and EML metadata\nDOIs issued for all datasets ensuring citability\nFull archive allowing for any data file types\nBasic spatial and temporal filtering through web interface\nDownload in original formats with metadata\nAccess through DataOne API\nLinks:\n\nEDIrepository.org\nCalCOFI datasets: EDI query “CalCOFI”\n\n\n\n\n6.3.2 NCEI\nNational Centers for Environmental Information\n\nLong-term archival of oceanographic data\nDOIs issued for dataset submissions\nStandardized metadata using ISO 19115-2\nBasic search interface with geographic and temporal filtering\nData preserved in original submission formats\nAccess through NCEI API services\nLinks:\n\nNCEI Ocean Archive\nCalCOFI datasets: NCEI search “CalCOFI”\n\n\n\n\n6.3.3 OBIS\nOcean Biodiversity Information System\n\nSpecialized in marine biodiversity data\nStandardized using DarwinCore fields\nExtended measurements supported via extendedMeasurementOrFact\nPowerful filtering by space, time, and taxonomic parameters\nMultiple download formats (CSV, JSON, Darwin Core Archive)\nFull REST API access\nLinks:\n\nOBIS.org\nCalCOFI datasets: obis.org/dataset + “calcofi” Keyword\n\n\n\n\n6.3.4 ERDDAP\nEnvironmental Research Division Data Access Program\n\nTabular and gridded data server\nAdvanced subsetting by space, time, and parameters\nMultiple output formats (CSV, JSON, NetCDF, etc.)\nRESTful API with direct data access\nBuilt-in data visualization tools\nNo persistent identifiers but stable URLs\nLinks:\n\nERDDAP\nCalCOFI datasets:\n\nERDDAP, OceanView - CalCOFI seabirds\nERDDAP, CoastWatch - CalCOFI oceanographic",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#metadata",
    "href": "portals.html#metadata",
    "title": "6  Portals",
    "section": "6.4 Metadata",
    "text": "6.4 Metadata\nThe Ecological Metadata Language (EML) (and using R package EML in workflows) serves as a key standard for describing ecological and environmental data. For CalCOFI, EML metadata files are generated alongside data files, providing structured documentation that enables interoperability across different data portals. This metadata-driven approach allows automated ingestion into various data systems while maintaining data integrity and provenance.\n\n\n\n\n\n\ngraph LR\n    subgraph dataset[Dataset]\n        data([data.csv]):::source\n        eml[metadata.eml]:::database\n    end\n\n    subgraph portals[Portals]\n        edi[EDI]:::portal\n        erddap[ERDDAP]:::portal\n        obis[OBIS]:::portal\n        ncei[NCEI]:::portal\n    end\n\n    subgraph metaportals[Meta-Portals]\n        odis[ODIS]:::metaportal\n        datagov[data.gov]:::metaportal\n    end\n\n    dataset --&gt; portals\n    eml --&gt;|json-ld| metaportals\n\n    %% Custom styles\n    classDef source     fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n    classDef database   fill:#FEF3C7,stroke:#D97706,stroke-width:2px\n    classDef portal     fill:#F0FDF4,stroke:#22C55E,stroke-width:2px\n    classDef metaportal fill:#FEF2F2,stroke:#DC2626,stroke-width:2px\n\n    %% Style subgraphs\n    style dataset     fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n    style portals     fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n    style metaportals fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n\n\n\n\nFigure 6.2: Metadata in the form of ecological metadata language (EML) is used to describe the dataset in a consistent manner that can be ingested by the portals.\n\n\n\n\n\nThe EML specification provides detailed structure for describing datasets, including:\n\nDataset identification and citation\nGeographic and temporal coverage\nVariable definitions and units\nMethods and protocols\nQuality control procedures\nAccess and usage rights\n\nThis standardized metadata enables automated data transformation and ingestion into various portal systems while preserving the original data context and quality information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#meta-portals",
    "href": "portals.html#meta-portals",
    "title": "6  Portals",
    "section": "6.5 Meta-Portals",
    "text": "6.5 Meta-Portals\n\n6.5.1 Google Dataset Search\nThe JSON-LD metadata in the Portal dataset web pages get indexed by Google Dataset Search through schema.org metadata. This ensures that CalCOFI data is discoverable through Google search and other search engines.\n\n\n6.5.2 ODIS\nOcean Data Information System\nODIS uses the same technology as Google Dataset Search (schema.org, JSON-LD), but focuses on ocean data. CalCOFI curates a sitemap of authoritative datasets to server to ODIS.org\nThis federated approach ensures that CalCOFI data remains:\n\nDiscoverable through multiple channels\nProperly cited and attributed\nIntegrated with global ocean data systems",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#calcofi.io-tools",
    "href": "portals.html#calcofi.io-tools",
    "title": "6  Portals",
    "section": "6.6 CalCOFI.io Tools",
    "text": "6.6 CalCOFI.io Tools\nCalCOFI is also developing an integrated database and tools that enable efficient data access and analysis:\n\n6.6.1 APIs\n\nRESTful endpoints for programmatic access\nFiltering by space, time, and taxonomic parameters\nRelationship queries across tables\nLinks:\n\napi.calcofi.io\ntile.calcofi.io\n\n\n\n\n6.6.2 Library\n\nDirect data access from R\nBuilt-in analysis functions\nIntegration with tidyverse ecosystem\nLink:\n\ncalcofi.io/calcofi4r\n\n\n\n\n6.6.3 Apps\n\nInteractive data exploration with Shiny applications\nUser-friendly interfaces\nSubset and download data\nLink:\n\ncalcofi.io, App button",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "7  References",
    "section": "",
    "text": "7.1 R packages",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "refs.html#r-packages",
    "href": "refs.html#r-packages",
    "title": "7  References",
    "section": "",
    "text": "API: plumber (Schloerke and Allen 2024)\ndocs: Quarto (Allaire and Dervieux 2024)\napps: Shiny (Chang et al. 2024)\n\n\n\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to Quarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://shiny.posit.co/.\n\n\nSchloerke, Barret, and Jeff Allen. 2024. Plumber: An API Generator for r. https://www.rplumber.io.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>References</span>"
    ]
  }
]