[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CalCOFI.io Docs",
    "section": "",
    "text": "1 Process\n\n\n\n\n\n\n\nFigure 1.1: CalCOFI data workflow.\n\n\n\nThe original raw data, most often in tabular format [e.g., comma-separated value (*.csv)], gets ingested into the database by R scripts that use functions and lookup data tables in the R package calcofi4r where functions are organized into Read, Analyze and Visualize concepts. The application programming interface (API) provides a program-language-agnostic public interface for rendering subsets of data and custom visualizations given a set of documented input parameters for feeding interactive applications (Apps) using Shiny (or any other web application framework) and reports using Rmarkdown (or any other report templating framework). Finally, R scripts will publish metadata (as Ecological Metadata Language) and data packages (e.g., in Darwin format) for discovery on a variety of data portals oriented around slicing the tabular or gridded data (ERDDAP), biogeographic analysis (OBIS), long-term archive (DataOne, NCEI) or metadata discovery (InPort). The database will be spatially enabled by PostGIS for summarizing any and all data by Areas of Interest (AoIs), whether pre-defined (e.g., sanctuaries, MPAs, counties, etc.) or arbitrary new areas. (Figure 1.1)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Process</span>"
    ]
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "2  Reports",
    "section": "",
    "text": "2.1 Sanctuaries",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "reports.html#sanctuaries",
    "href": "reports.html#sanctuaries",
    "title": "2  Reports",
    "section": "",
    "text": "Channel Islands WebCR\nweb-enabled Condition Report\n\nForage Fish\nexample of using calcofi4r functions that pull from the API\n\nUCSB Student Capstone",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "3  Applications",
    "section": "",
    "text": "CalCOFI Oceanography\noceanographic summarization by arbitrary area of interest and sampling period\nUCSB Student Capstone",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "4  API",
    "section": "",
    "text": "4.1 /variables: get list of variables for timeseries\nThe raw interface to the Application Programming Interface (API) is available at:\nHere we will provide more guidance on how to use the API functions with documented input arguments, output results and examples of use.\nGet list of variables for use in /timeseries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#species_groups-get-species-groups-for-larvae",
    "href": "api.html#species_groups-get-species-groups-for-larvae",
    "title": "4  API",
    "section": "4.2 /species_groups: get species groups for larvae",
    "text": "4.2 /species_groups: get species groups for larvae\nNot yet working. Get list of species groups for use with variables larvae_counts.count in /timeseries",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#timeseries-get-time-series-data",
    "href": "api.html#timeseries-get-time-series-data",
    "title": "4  API",
    "section": "4.3 /timeseries: get time series data",
    "text": "4.3 /timeseries: get time series data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruises-get-list-of-cruises",
    "href": "api.html#cruises-get-list-of-cruises",
    "title": "4  API",
    "section": "4.4 /cruises: get list of cruises",
    "text": "4.4 /cruises: get list of cruises\nGet list of cruises with summary stats as CSV table for time (date_beg)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#raster-get-raster-map-of-variable",
    "href": "api.html#raster-get-raster-map-of-variable",
    "title": "4  API",
    "section": "4.5 /raster: get raster map of variable",
    "text": "4.5 /raster: get raster map of variable\nGet raster of variable",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruise_lines-get-station-lines-from-cruises",
    "href": "api.html#cruise_lines-get-station-lines-from-cruises",
    "title": "4  API",
    "section": "4.6 /cruise_lines: get station lines from cruises",
    "text": "4.6 /cruise_lines: get station lines from cruises\nGet station lines from cruises (with more than one cast)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "api.html#cruise_line_profile",
    "href": "api.html#cruise_line_profile",
    "title": "4  API",
    "section": "4.7 /cruise_line_profile",
    "text": "4.7 /cruise_line_profile\nGet profile at depths for given variable of casts along line of stations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API</span>"
    ]
  },
  {
    "objectID": "db.html",
    "href": "db.html",
    "title": "5  Database",
    "section": "",
    "text": "5.1 Database naming conventions\nWe’re circling the wagons to come up with the best conventions for naming. Here are some ideas:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#database-naming-conventions",
    "href": "db.html#database-naming-conventions",
    "title": "5  Database",
    "section": "",
    "text": "There are only two hard things in Computer Science: cache invalidation and naming things. – Phil Karlton (Netscape architect)\n\n\n\nLearn SQL: Naming Conventions\nBest Practices for Database Naming Conventions - Drygast.NET\n\n\n5.1.1 Name tables\n\nTable names are singular and use all lower case.\n\n\n\n5.1.2 Name columns\n\nTo name columns, use snake-case (i.e., lower-case with underscores) so as to prevent the need to quote SQL statements. (TIP: Use janitor::clean_names() to convert a table.)\nUnique identifiers are suffixed with:\n\n*_id for unique integer keys;\n*_uuid for universally unique identifiers as defined by RFC 4122 and stored in Postgres as UUID Type.\n*_key for unique string keys;\n*_seq for auto-incrementing sequence integer keys.\n\nSuffix with units where applicable (e.g., *_m for meters, *_km for kilometers, degc for degrees Celsius). See units vignette.\nSet geometry column to geom (used by PostGIS spatial extension). If the table has multiple geometry columns, use geom for the default geometry column and geom_{type} for additional geometry columns (e.g., geom_point, geom_line, geom_polygon).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#use-unicode-for-text",
    "href": "db.html#use-unicode-for-text",
    "title": "5  Database",
    "section": "5.2 Use Unicode for text",
    "text": "5.2 Use Unicode for text\nThe default character encoding for Postgresql is unicode (UTF8), which allows for international characters, accents and special characters. Improper encoding can royally mess up basic text.\nLogging into the server, we can see this with the following command:\ndocker exec -it postgis psql -l\n                                  List of databases\n        Name        | Owner | Encoding |  Collate   |   Ctype    | Access privileges \n--------------------+-------+----------+------------+------------+-------------------\n gis                | admin | UTF8     | en_US.utf8 | en_US.utf8 | =Tc/admin        +\n                    |       |          |            |            | admin=CTc/admin  +\n                    |       |          |            |            | ro_user=c/admin\n lter_core_metabase | admin | UTF8     | en_US.utf8 | en_US.utf8 | =Tc/admin        +\n                    |       |          |            |            | admin=CTc/admin  +\n                    |       |          |            |            | rw_user=c/admin\n postgres           | admin | UTF8     | en_US.utf8 | en_US.utf8 | \n template0          | admin | UTF8     | en_US.utf8 | en_US.utf8 | =c/admin         +\n                    |       |          |            |            | admin=CTc/admin\n template1          | admin | UTF8     | en_US.utf8 | en_US.utf8 | =c/admin         +\n                    |       |          |            |            | admin=CTc/admin\n template_postgis   | admin | UTF8     | en_US.utf8 | en_US.utf8 | \n(6 rows)\nUse Unicode (utf-8 in Python or UTF8 in Postgresql) encoding for all database text values to support international characters and documentation (i.e., tabs, etc for markdown conversion).\n\nIn Python, use pandas to read (read_csv()) and write (to_csv()) with UTF-8 encoding (i.e., encoding='utf-8').:\nimport pandas as pd\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://user:password@localhost:5432/dbname')\n\n# read from a csv file\ndf = pd.read_csv('file.csv', encoding='utf-8')\n\n# write to PostgreSQL\ndf.to_sql('table_name', engine, if_exists='replace', index=False, method='multi', chunksize=1000, encoding='utf-8')\n\n# read from PostgreSQL\ndf = pd.read_sql('SELECT * FROM table_name', engine, encoding='utf-8')\n\n# write to a csv file with UTF-8 encoding\ndf.to_csv('file.csv', index=False, encoding='utf-8')\nIn R, use readr to read (read_csv()) and write (write_excel_csv()) to force UTF-8 encoding.\nlibrary(readr)\nlibrary(DBI)\nlibrary(RPostgres)\n\n# connect to PostgreSQL\ncon &lt;- dbConnect(RPostgres::Postgres(), dbname = \"dbname\", host = \"localhost\", port = 5432, user = \"user\", password = \"password\")\n\n# read from a csv file\ndf &lt;- read_csv('file.csv', locale = locale(encoding = 'UTF-8'))  # explicit\ndf &lt;- read_csv('file.csv')                                       # implicit\n\n# write to PostgreSQL\ndbWriteTable(con, 'table_name', df, overwrite = TRUE)\n\n# read from PostgreSQL\ndf &lt;- dbReadTable(con, 'table_name')\n\n# write to a csv file with UTF-8 encoding\nwrite_excel_csv(df, 'file.csv', locale = locale(encoding = 'UTF-8'))  # explicit\nwrite_excel_csv(df, 'file.csv')                                       # implicit",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#integrated-database-ingestion-strategy",
    "href": "db.html#integrated-database-ingestion-strategy",
    "title": "5  Database",
    "section": "5.3 Integrated database ingestion strategy",
    "text": "5.3 Integrated database ingestion strategy\n\n5.3.1 Overview\nThe CalCOFI database uses a two-schema strategy for development and production:\n\ndev schema: Development schema where new datasets, tables, fields, and relationships are ingested and QA/QC’d. This schema is recreated fresh with each ingestion run using the master ingestion script.\nprod schema: Production schema for stable, versioned data used by public APIs, apps, and data portals (OBIS, EDI, ERDDAP). Once dev is validated, it’s copied to prod with a version number.\n\n\n\n5.3.2 Master ingestion workflow\nAll datasets are ingested using a single master Quarto script calcofi4db/inst/create_db.qmd that:\n\nDrops and recreates the dev schema (fresh start each run)\nIngests multiple datasets from Google Drive source files (CSV, potentially SHP/NC in future)\nApplies transformations using redefinition files (tbls_redefine.csv, flds_redefine.csv)\nCreates relationships (primary keys, foreign keys, indexes)\nRecords schema version with metadata in schema_version table\n\nEach dataset section in the master script handles:\n\nReading CSV files from Google Drive\nTransforming data according to redefinition rules\nLoading into database tables\nAdding table/field comments with metadata\n\n\n\n\n\n\n\nflowchart TB\n    %% Node definitions\n    gd[(\"`&lt;b&gt;Source Data&lt;/b&gt;\n          Google Drive:\n          calcofi/data/{provider}/{dataset}/*.csv`\")]\n    iw[\"&lt;b&gt;Ingest&lt;/b&gt;\n        calcofi4db: create_db.qmd; sections by {provider},{dataset}\"]\n    dd[\"&lt;b&gt;Data Definitions&lt;/b&gt;\n        calcofi4db: ingest/{provider}/{dataset}/:\n        &lt;ul&gt;\n          &lt;li&gt;tbls_redefine.csv&lt;/li&gt;\n          &lt;li&gt;flds_redefine.csv&lt;/li&gt;\n        &lt;/ul&gt;\"]\n    db[(\"&lt;b&gt;Database&lt;/b&gt;\")]\n    api[\"&lt;b&gt;API Endpoint&lt;/b&gt;\n         /db_tables\n         /db_columns\"]\n    catalog[\"&lt;b&gt;R Function&lt;/b&gt;\n             calcofi4r::cc_db_catalog()\"]\n    eml[\"&lt;b&gt;Publish Workflow&lt;/b&gt;\n      workflows: publish_{dataset}_{portal}.qmd\n      with {portal}s:\n      &lt;ul&gt;\n        &lt;li&gt;erddap&lt;/li&gt;\n        &lt;li&gt;edi&lt;/li&gt;\n        &lt;li&gt;obis&lt;/li&gt;\n        &lt;li&gt;ncei&lt;/li&gt;\n      &lt;/ul&gt;\"]\n\n    %% Edge definitions\n    gd --&gt; iw\n    iw --&gt;|\"1 auto-generated\"| dd\n    dd --&gt;|\"2 manual edit\"| iw\n    iw --&gt;|\"3 data\"| db\n    iw --&gt; comments\n    comments --&gt;|\"4 metadata\"| db\n    db --&gt; api\n    api --&gt; catalog\n    db --&gt; eml\n\n    %% Comments subgraph with internal nodes\n    subgraph comments[\"&lt;b&gt;Database Comments&lt;/b&gt;\n              (stored as text in JSON format to differentiate elements)\"]\n        direction TB\n        h[\"hideme\"]:::hidden\n        h~~~tbl\n        h~~~fld\n        tbl[\"per &lt;em&gt;Table&lt;/em&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;description&lt;/li&gt;\n              &lt;li&gt;source (&lt;em&gt;linked&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;source_created (&lt;em&gt;datetime&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;workflow (&lt;em&gt;linked&lt;/em&gt;)&lt;/li&gt;\n              &lt;li&gt;workflow_ingested (&lt;em&gt;datetime&lt;/em&gt;)&lt;/li&gt;\n            &lt;/ul&gt;\"]\n        fld[\"per &lt;em&gt;Field&lt;/em&gt;:\n            &lt;ul&gt;\n              &lt;li&gt;description&lt;/li&gt;\n              &lt;li&gt;units (SI)`&lt;/li&gt;\n            &lt;/ul&gt;\"]\n    end\n\n    %% Clickable links\n    click gd \"https://drive.google.com/drive/folders/1xxdWa4mWkmfkJUQsHxERTp9eBBXBMbV7\" \"calcofi folder - Google Drive\"\n    click api \"https://api.calcofi.io/db_tables\" \"API endpoint&lt;/b&gt;\"\n    click catalog \"https://calcofi.io/calcofi4r/reference/cc_db_catalog.html\" \"R package function\"\n\n    %% Styling\n    classDef source fill:#f9f9f9,stroke:#000,stroke-width:2px,color:#000\n    classDef process fill:#a3e0f2,stroke:#000,stroke-width:2px,color:#000\n    classDef eml fill:#F0FDF4,stroke:#22C55E,stroke-width:2px,color:#000,text-align:left\n    classDef data fill:#ffbe75,stroke:#000,stroke-width:2px,color:#000\n    classDef api fill:#9ad294,stroke:#000,stroke-width:2px,color:#000\n    classDef meta fill:#c9a6db,stroke:#000,stroke-width:2px,color:#000,text-align:left\n    classDef hidden display: none;\n\n    class gd source\n    class dd,comments,tbl,fld meta\n    class iw process\n    class db data\n    class api,catalog api\n    class tbl,fld li\n    class eml eml\n\n\n\n\nFigure 5.1: Integrated database ingestion scheme.\n\n\n\n\n\n\n\n5.3.3 Using calcofi4db package\nThe calcofi4db package provides streamlined functions for dataset ingestion:\nlibrary(calcofi4db)\nlibrary(DBI)\nlibrary(RPostgres)\n\n# Connect to database\ncon &lt;- dbConnect(\n  Postgres(),\n  dbname = \"gis\",\n  host = \"localhost\",\n  port = 5432,\n  user = \"admin\",\n  password = \"postgres\"\n)\n\n# Read CSV files and metadata\nd &lt;- read_csv_files(\n  provider = \"swfsc.noaa.gov\",\n  dataset = \"calcofi-db\"\n)\n\n# Transform data according to redefinitions\ntransformed_data &lt;- transform_data(d)\n\n# Ingest into dev schema\ningest_csv_to_db(\n  con = con,\n  schema = \"dev\",\n  transformed_data = transformed_data,\n  d_flds_rd = d$d_flds_rd,\n  d_gdata = d$d_gdata,\n  workflow_info = d$workflow_info\n)\n\n# Record schema version\nrecord_schema_version(\n  con = con,\n  schema = \"dev\",\n  version = \"1.0.0\",\n  description = \"Initial ingestion of NOAA CalCOFI Database\",\n  script_permalink = \"https://github.com/CalCOFI/calcofi4db/blob/main/inst/create_db.qmd\"\n)\n\n\n5.3.4 Schema versioning\nEach successful ingestion creates a new schema version recorded in the schema_version table with:\n\nversion: Semantic version number (e.g., “1.0.0”, “1.1.0”)\ndescription: Changes introduced in this version\ndate_created: Timestamp of ingestion\nscript_permalink: GitHub permalink to the versioned ingestion script\n\nVersions are also archived as SQL dumps in Google Drive for reproducibility.\n\n\n5.3.5 Metadata and documentation\nAfter ingestion, metadata is stored in PostgreSQL COMMENTs as JSON at the table level:\n\ndescription: General description and row uniqueness\nsource: CSV file link to Google Drive\nsource_created: Source file creation timestamp\nworkflow: Link to rendered ingestion script\nworkflow_ingested: Ingestion timestamp\n\nAnd at the field level:\n\ndescription: Field description\nunits: SI units where applicable\n\nThese comments are exposed via the API db_tables endpoint and rendered with calcofi4r::cc_db_catalog.\n\n\n5.3.6 Publishing to portals\nAfter prod schema is versioned, additional workflows publish data to Portals (ERDDAP, EDI, OBIS, NCEI) using ecological metadata language (EML) via the EML R package, pulling metadata directly from database comments.\n\n\n5.3.7 OR Describe tables and columns directly\n\nUse the COMMENT clause to add descriptions to tables and columns, either through the GUI pgadmin.calcofi.io (by right-clicking on the table or column and selecting Properties) or with SQL. For example:\nCOMMENT ON TABLE public.aoi_fed_sanctuaries IS 'areas of interest (`aoi`) polygons for federal **National Marine Sanctuaries**; loaded by _workflow_ [load_sanctuaries](https://calcofi.io/workflows/load_sanctuaries.html)';\nNote the use of markdown for including links and formatting (e.g., bold, code, italics), such that the above SQL will render like so:\n\nareas of interest (aoi) polygons for federal National Marine Sanctuaries; loaded by workflow load_sanctuaries\n\nIt is especially helpful to link to any workflows that are responsible for the ingesting or updating of the input data.\n\n\n\n5.3.8 Display tables and columns with metadata\n\nThese descriptions can be viewed in the CalCOFI API api.calcofi.io as CSV tables (see code in calcofi/api: plumber.R):\n\napi.calcofi.io/db_tables\nfields:\n\n\nschema: (only “public” so far)\ntable_type: “table”, “view”, or “materialized view” (none yet)\ntable: name of table\ntable_description: description of table (possibly in markdown)\n\napi.calcofi.io/db_columns\nfields:\n\n\nschema: (only “public” so far)\ntable_type: “table”, “view”, or “materialized view” (none yet)\ntable: name of table\ncolumn: name of column\ncolumn_type: data type of column\ncolumn_description: description of column (possibly in markdown)\n\n\nFetch and display these descriptions into an interactive table with calcofi4r::cc_db_catalog().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#relationships-between-tables",
    "href": "db.html#relationships-between-tables",
    "title": "5  Database",
    "section": "5.4 Relationships between tables",
    "text": "5.4 Relationships between tables\n\nSee calcofi/workflows: clean_db\n\nTODO: add calcofi/apps: db to show latest tables, columns and relationsips",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "db.html#spatial-tips",
    "href": "db.html#spatial-tips",
    "title": "5  Database",
    "section": "5.5 Spatial Tips",
    "text": "5.5 Spatial Tips\n\nUse ST_Subdivide() when running spatial joins on large polygons.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Database</span>"
    ]
  },
  {
    "objectID": "portals.html",
    "href": "portals.html",
    "title": "6  Portals",
    "section": "",
    "text": "6.1 Overview\nCalCOFI data is available through various portals, each serving different purposes and user needs. This document outlines the main access points and their characteristics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#data-flow",
    "href": "portals.html#data-flow",
    "title": "6  Portals",
    "section": "6.2 Data Flow",
    "text": "6.2 Data Flow\nWhile it would be ideal for CalCOFI data to be available through a single portal, each portal has its strengths and limitations. The following diagram illustrates one possible realization of data flow between CalCOFI data and the portals: from raw data to the integrated database to portals and meta-portals.\n\n\n\n\n\n\ngraph TD\n  %% nodes with styles\n  raw([raw data]):::source\n\n  subgraph calcofi[CalCOFI.io]\n    db[(database)]:::database\n    web[CalCOFI.org]:::website\n    api[APIs]:::api\n    lib[library]:::code\n    app[apps]:::code\n    flow[workflows]:::code\n  end\n\n  subgraph portals[Portals]\n    edi[EDI]:::portal\n    obis[OBIS]:::portal\n    erddap[ERDDAP]:::portal\n    ncei[NCEI]:::portal\n  end\n\n  subgraph meta[Meta-Portals]\n    odis[ODIS]:::metaportal\n    datagov[data.gov]:::metaportal\n  end\n\n  %% edges\n  raw --&gt; db\n  db  --&gt; api\n  db  --&gt; web\n  api --&gt; lib\n  api --&gt; app\n  api --&gt; flow\n  flow  --&gt; portals\n  portals --&gt;|sitemap| odis\n  portals --&gt; datagov\n\n  %% Custom styles\n  classDef source     fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n  classDef database   fill:#FEF3C7,stroke:#D97706,stroke-width:2px\n  classDef website    fill:#F3E8FF,stroke:#9333EA,stroke-width:2px\n  classDef api        fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n  classDef code       fill:#DBEAFE,stroke:#3B82F6,stroke-width:2px\n  classDef portal     fill:#F0FDF4,stroke:#22C55E,stroke-width:2px\n  classDef metaportal fill:#FEF2F2,stroke:#DC2626,stroke-width:2px\n\n  %% Style subgraphs\n  style calcofi fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n  style portals fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n  style meta fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n\n\n\n\nFigure 6.1: Flow of data from raw to database to portals and meta-portals.\n\n\n\n\n\nIn practice, CalCOFI is a partnership with various contributing members, so the authoritative dataset might flow differently, such as from EDI to the database to the other portals. The other portals, such as OBIS or ERDDAP, serve different audiences or purposes. The meta-portals like ODIS and Data.gov then index these portals to provide broader discovery of CalCOFI datasets.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#portals",
    "href": "portals.html#portals",
    "title": "6  Portals",
    "section": "6.3 Portals",
    "text": "6.3 Portals\nWhile some portals serve as data repositories, others provide advanced data access and visualization tools. The following sections describe the main portals where CalCOFI data is available and their key features.\n\n\n\n\nTable 6.1: Portal Capabilities.\n\n\n\n\n\n\n\n\n\n\nFull Archive\nVersioning\nDOI Issued\nQuery by xyt\nQuery by taxa\nMultiple formats\nAPI Access\n\n\n\n\nEDI\n✔\n✔\n✔\n▲\n▲\n✖\n▲\n\n\nNCEI\n✔\n✔\n✔\n✖\n✖\n✖\n▲\n\n\nOBIS\n▲\n▲\n▲\n✔\n✔\n▲\n✔\n\n\nERDDAP\n▲\n✖\n✖\n✔\n▲\n✔\n✔\n\n\n\nCapability Legend: ✔ = full, ▲ = partial, ✖ = none\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.1 EDI\nEnvironmental Data Initiative\n\nComplete dataset archives using DataOne software and EML metadata\nDOIs issued for all datasets ensuring citability\nFull archive allowing for any data file types\nBasic spatial and temporal filtering through web interface\nDownload in original formats with metadata\nAccess through DataOne API\nLinks:\n\nEDIrepository.org\nCalCOFI datasets: EDI query “CalCOFI”\n\n\n\n\n6.3.2 NCEI\nNational Centers for Environmental Information\n\nLong-term archival of oceanographic data\nDOIs issued for dataset submissions\nStandardized metadata using ISO 19115-2\nBasic search interface with geographic and temporal filtering\nData preserved in original submission formats\nAccess through NCEI API services\nLinks:\n\nNCEI Ocean Archive\nCalCOFI datasets: NCEI search “CalCOFI”\n\n\n\n\n6.3.3 OBIS\nOcean Biodiversity Information System\n\nSpecialized in marine biodiversity data\nStandardized using DarwinCore fields\nExtended measurements supported via extendedMeasurementOrFact\nPowerful filtering by space, time, and taxonomic parameters\nMultiple download formats (CSV, JSON, Darwin Core Archive)\nFull REST API access\nLinks:\n\nOBIS.org\nCalCOFI datasets: obis.org/dataset + “calcofi” Keyword\n\n\n\n\n6.3.4 ERDDAP\nEnvironmental Research Division Data Access Program\n\nTabular and gridded data server\nAdvanced subsetting by space, time, and parameters\nMultiple output formats (CSV, JSON, NetCDF, etc.)\nRESTful API with direct data access\nBuilt-in data visualization tools\nNo persistent identifiers but stable URLs\nLinks:\n\nERDDAP\nCalCOFI datasets:\n\nERDDAP, OceanView - CalCOFI seabirds\nERDDAP, CoastWatch - CalCOFI oceanographic",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#metadata",
    "href": "portals.html#metadata",
    "title": "6  Portals",
    "section": "6.4 Metadata",
    "text": "6.4 Metadata\nThe Ecological Metadata Language (EML) (and using R package EML in workflows) serves as a key standard for describing ecological and environmental data. For CalCOFI, EML metadata files are generated alongside data files, providing structured documentation that enables interoperability across different data portals. This metadata-driven approach allows automated ingestion into various data systems while maintaining data integrity and provenance.\n\n\n\n\n\n\ngraph LR\n    subgraph dataset[Dataset]\n        data([data.csv]):::source\n        eml[metadata.eml]:::database\n    end\n\n    subgraph portals[Portals]\n        edi[EDI]:::portal\n        erddap[ERDDAP]:::portal\n        obis[OBIS]:::portal\n        ncei[NCEI]:::portal\n    end\n\n    subgraph metaportals[Meta-Portals]\n        odis[ODIS]:::metaportal\n        datagov[data.gov]:::metaportal\n    end\n\n    dataset --&gt; portals\n    eml --&gt;|json-ld| metaportals\n\n    %% Custom styles\n    classDef source     fill:#E0E7FF,stroke:#6366F1,stroke-width:2px\n    classDef database   fill:#FEF3C7,stroke:#D97706,stroke-width:2px\n    classDef portal     fill:#F0FDF4,stroke:#22C55E,stroke-width:2px\n    classDef metaportal fill:#FEF2F2,stroke:#DC2626,stroke-width:2px\n\n    %% Style subgraphs\n    style dataset     fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n    style portals     fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n    style metaportals fill:#F8FAFC,stroke:#CBD5E1,stroke-width:2px\n\n\n\n\nFigure 6.2: Metadata in the form of ecological metadata language (EML) is used to describe the dataset in a consistent manner that can be ingested by the portals.\n\n\n\n\n\nThe EML specification provides detailed structure for describing datasets, including:\n\nDataset identification and citation\nGeographic and temporal coverage\nVariable definitions and units\nMethods and protocols\nQuality control procedures\nAccess and usage rights\n\nThis standardized metadata enables automated data transformation and ingestion into various portal systems while preserving the original data context and quality information.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#meta-portals",
    "href": "portals.html#meta-portals",
    "title": "6  Portals",
    "section": "6.5 Meta-Portals",
    "text": "6.5 Meta-Portals\n\n6.5.1 Google Dataset Search\nThe JSON-LD metadata in the Portal dataset web pages get indexed by Google Dataset Search through schema.org metadata. This ensures that CalCOFI data is discoverable through Google search and other search engines.\n\n\n6.5.2 ODIS\nOcean Data Information System\nODIS uses the same technology as Google Dataset Search (schema.org, JSON-LD), but focuses on ocean data. CalCOFI curates a sitemap of authoritative datasets to server to ODIS.org\nThis federated approach ensures that CalCOFI data remains:\n\nDiscoverable through multiple channels\nProperly cited and attributed\nIntegrated with global ocean data systems",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "portals.html#calcofi.io-tools",
    "href": "portals.html#calcofi.io-tools",
    "title": "6  Portals",
    "section": "6.6 CalCOFI.io Tools",
    "text": "6.6 CalCOFI.io Tools\nCalCOFI is also developing an integrated database and tools that enable efficient data access and analysis:\n\n6.6.1 APIs\n\nRESTful endpoints for programmatic access\nFiltering by space, time, and taxonomic parameters\nRelationship queries across tables\nLinks:\n\napi.calcofi.io\ntile.calcofi.io\n\n\n\n\n6.6.2 Library\n\nDirect data access from R\nBuilt-in analysis functions\nIntegration with tidyverse ecosystem\nLink:\n\ncalcofi.io/calcofi4r\n\n\n\n\n6.6.3 Apps\n\nInteractive data exploration with Shiny applications\nUser-friendly interfaces\nSubset and download data\nLink:\n\ncalcofi.io, App button",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portals</span>"
    ]
  },
  {
    "objectID": "status.html",
    "href": "status.html",
    "title": "7  Status",
    "section": "",
    "text": "8 Status",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Status</span>"
    ]
  },
  {
    "objectID": "status.html#section",
    "href": "status.html#section",
    "title": "7  Status",
    "section": "8.1 2025-12-01",
    "text": "8.1 2025-12-01\n\nOver the past several months, CalCOFI’s software and data systems have advanced significantly toward a unified, reliable, and user‑friendly platform for exploring and publishing CalCOFI data. Work has focused on four main areas: the integrated data app, the underlying database and workflows, pushing to OBIS, and organizing of CTD data.\n\n\n8.1.0.1 1. A More Capable, User-Friendly Integrated App\nThe CalCOFI integrated application (int-app) has evolved into a much richer and more intuitive tool for scientists and partners:\n\nTaxonomy-aware exploration\nThe app now understands species and their taxonomic relationships. Users can browse taxa hierarchies, see taxonomic ranks, and work with improved species metadata tied to authoritative sources (e.g., WoRMS, ITIS). This makes it easier to find and compare species and groups of species consistently.\nBetter visual experience and theming\nA new dark/light theme toggle has been implemented and refined so that maps, time series, and other plots remain readable and visually consistent. Navigation has been reorganized, with a clearer About page, a guided “tour” of the app, and more intuitive icons and labels, making the app easier to learn and use.\nStronger spatial and temporal tools\nSpatial maps now rely on efficient hexagon grids calculated in the database, improving performance and scalability. Default settings for time and depth matching have been tuned to yield better joins between environmental and biological data out of the box.\n\nOverall, the app is moving from a prototype to a polished, guided interface that better supports exploratory analysis and communication.\n\n\n\n8.1.0.2 2. A Stable, Well-Documented Database Foundation\nThe CalCOFI database package (calcofi4db) has been formalized and versioned, providing a solid foundation for all downstream tools:\n\nTwo stable releases (versions 1.0 and 1.1) have established a reliable baseline for the database, including bottle-level data.\nThe project now follows a clear strategy for separate development and production databases, reducing risk when making changes and improving reproducibility.\nData ingestion from NOAA and other sources has been hardened, with several rounds of fixes to handle edge cases and ensure that raw files are consistently and correctly translated into the database.\n\nIn addition, the package’s online documentation site has been refreshed so that developers and analysts have up-to-date guidance on how data flow into and through the database.\n\n\n\n8.1.0.3 3. Unified R Tools and Documentation Around DuckDB\nAcross the toolchain, CalCOFI has standardized on DuckDB as the core data engine:\n\nThe R package (calcofi4r) now encapsulates key logic originally developed inside the integrated app, so the same high-quality data access and processing is available in scripts, reports, and analyses—not just in the web interface.\nBoth the app and R package can connect to local or remote DuckDB databases, improving performance and enabling offline or near‑offline workflows.\nDocumentation in the docs repository has been updated to describe the full data creation process (from raw data to ready‑to‑use databases) and to explain the new development/production database strategy. Status documents and helper scripts provide clearer visibility into project progress.\n\nThis brings CalCOFI closer to a coherent, documented platform where analysts can move seamlessly between app-based exploration and scripted analysis.\n\n\n\n8.1.0.4 4. Standards-Compliant Publication of Biological Data\nThe workflows repository has seen major progress in turning CalCOFI’s biological datasets into publication-ready products:\n\nNew workflows now publish larval data to OBIS, the global biodiversity information system, with repeatable recipes that combine biological observations with CTD (oceanographic) data.\nThe underlying data model for events and occurrences has been strengthened to align with international standards (e.g., Darwin Core), including:\n\nClear hierarchies of sampling events,\nBetter handling of life‑stage and size information (e.g., egg and larval stages),\nAutomated generation of metadata files required for data archives.\n\nAdditional integrity checks and foreign key relationships help ensure that data are correct and consistent before publication.\n\nThese advances substantially improve CalCOFI’s ability to share high-quality, well-structured biodiversity data with the broader scientific community.\n\n\n\n8.1.0.5 5. Improved Public Access and Infrastructure\nFinally, several changes improve how external users find and access CalCOFI tools:\n\nThe public website (CalCOFI.github.io) now highlights key applications, including the integrated app and a pollutants-focused app, making them easier to discover.\nServer configuration has been updated so that Shiny apps are served from a new dedicated domain app.calcofi.io (and still shiny.calcofi.io), clarifying the entry point for interactive tools and simplifying operations.\n\n\n\n\n8.1.1 Overall Impact\nTogether, these developments move CalCOFI toward a modern, integrated data platform:\n\nScientists and partners gain a more powerful, user‑friendly app and R toolkit for exploring CalCOFI data.\nThe underlying database and workflows are more robust, testable, and clearly documented.\nCalCOFI’s biological data are better positioned for global visibility and reuse through standards‑compliant publication channels like OBIS.\nPublic‑facing web presence and infrastructure are cleaner and more aligned, making it easier for stakeholders to find and use CalCOFI resources.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Status</span>"
    ]
  },
  {
    "objectID": "status.html#section-1",
    "href": "status.html#section-1",
    "title": "7  Status",
    "section": "8.2 2025-07-01",
    "text": "8.2 2025-07-01\n\nThis report summarizes the key development activities, major accomplishments, and ongoing work for the first 6 monhts of 2025 across the CalCOFI GitHub repositories: api, apps, calcofi4db, calcofi4r, docs, server, workflows. The findings are based on issues and commits from January–July 2025.\n\n\n8.2.1 API Enhancements\n\n8.2.1.1 New Features & Data Integration\n\nExpanded API Options\n\nAdded ability to include bottle data and use relaxed criteria for net-to-cast matching (commit).\nSupported upcast/downcast data downloads (commit).\nAdded Zooplankton biomass and improved ichthyodata output (commit).\n\nPerformance & Maintenance\n\nImplemented docker compose restart for Plumber API service (commit).\n\nOngoing Work\n\nMigration of database contouring functions to API/app level for improved caching and rendering efficiency.\nDevelopment of a robust, user-friendly API for seamless DB integration (issue).\n\n\n\n\n\n\n8.2.2 Apps Development\n\n8.2.2.1 Visualization & User Interface\n\nContinuous Improvements\n\nMultiple commits indicate ongoing enhancement, likely focused on UI, data visualization, and integration with the API (see recent commit log).\nClose coordination between API and Apps for improved workflows and data access.\n\n\n\n\n\n\n8.2.3 calcofi4db: R Package & Data Management\n\n8.2.3.1 R Package Initialization & Data Ingestion\n\nNew R Package: calcofi4db\n\nInitial commit and setup (commit), including functions for ingesting CSV datasets and metadata.\nRefined change detection logic for source CSV files, improving tracking of table/field changes (commit).\nEnhanced documentation and site via pkgdown.\nImproved function naming and structure for ingestion (commits, commit).\n\n\n\n\n\n\n8.2.4 calcofi4r: Spatial & Ecological Data Tools\n\n8.2.4.1 Data Layers, Analysis, and Bug Fixes\n\nSpatial Management Layers\n\nOngoing integration of BOEM Wind Planning Areas, Marine Protected Areas, and SCCWRP management regions (issue, issue).\n\nAnalysis Functions\n\nImproved packages for ecological and spatial analysis, including new dependencies (commit).\n\nUser Feedback\n\nAddressing user-reported bugs such as deprecated function calls (issue).\n\n\n\n\n\n\n8.2.5 Documentation (docs)\n\n8.2.5.1 Infrastructure & Environment\n\nDocumentation Site Updates\n\nAdded documentation for new packages and ingestion workflows (commit).\nImproved environment handling for rendering with Quarto and Chromium (multiple commits Jan-Mar 2025).\nUpdated diagrams and edge labels for database documentation.\n\n\n\n\n\n\n8.2.6 Server\n\n8.2.6.1 Backend Infrastructure\n\nBackend Maintenance\n\nNumerous commits for improving server reliability, configuration, and deployment.\nIndicates active backend support for API and Apps.\n\n\n\n\n\n\n8.2.7 Workflows\n\n8.2.7.1 Data Pipeline, Integration, and Registration\n\nWorkflow Automation\n\nMultiple commits show ongoing development of data ingestion, harmonization, and visualization workflows (commit, commit).\n\nODIS Registration\n\nRegistering datasets with ODIS (using JSON-LD) for broader interoperability (issue).\n\nIntegration with External Data\n\nOngoing work to load and harmonize diverse ecological datasets (bottle data, larvae, zooplankton, etc.).\n\nSpatial Data Management\n\nContinued development of AOI (areas of interest), spatial buffer creation, and integration of management regions.\n\n\n\n\n\n\n8.2.8 Key Themes & Impact\n\n8.2.8.1 Integration & Interoperability\n\nStrong focus on connecting API, Apps, R packages, and backend infrastructure for seamless data access and visualization.\nEnhanced interoperability through ODIS registration and harmonized workflows.\n\n\n\n8.2.8.2 Data Accessibility & Usability\n\nImprovements to API and Apps make ecological data more accessible to researchers and managers.\nExpanded support for spatial management areas and ecological datasets.\n\n\n\n8.2.8.3 Infrastructure & Sustainability\n\nInvestments in documentation, backend reliability, and workflow automation contribute to long-term sustainability and reproducibility.\n\n\n\n\n\n8.2.9 For More Details\n\nSome results may be incomplete due to API limits.\n\nTo view all commits/issues for 2025, visit each repository’s GitHub UI and filter by year.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Status</span>"
    ]
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "8  References",
    "section": "",
    "text": "8.1 R packages",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "refs.html#r-packages",
    "href": "refs.html#r-packages",
    "title": "8  References",
    "section": "",
    "text": "API: plumber (Schloerke and Allen 2024)\ndocs: Quarto (Allaire and Dervieux 2024)\napps: Shiny (Chang et al. 2024)\n\n\n\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to Quarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://shiny.posit.co/.\n\n\nSchloerke, Barret, and Jeff Allen. 2024. Plumber: An API Generator for r. https://www.rplumber.io.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>References</span>"
    ]
  }
]